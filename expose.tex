\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{glossaries}

\DeclareUnicodeCharacter{202F}{'e}

\newacronym{shex}{ShEx}{Shape Expressions}
\newacronym{rdf}{RDF}{Resource Description Framework}
\newacronym{aifb}{AIFB}{Institute of Applied Informatics and Formal Description Methods}
\newacronym{wmde}{WMDE}{Wikimedia Deutschland e.\,V.}

\makeglossaries

\title{Schema Inference and Validation on Wikidata}
\subtitle{Exposé}
\author{Lucas Werkmeister}

\begin{document}

\maketitle

\section{Goal}

The main goal of this thesis will be to use data schemata for quality control on Wikidata items.
This will include developing a way to infer such schemata from a set of exemplary items:
for example, given some composers, some capitals, or some chemical elements,
one could infer that all composers have a statement “instance of: human”;
that all capitals have a statement “instance of: (some subclass of) city”;
and that most chemical elements have a “mass” statement, but with varying values.
These schemata can then be used to validate other Wikidata items.
A maintenance scheme,
which is required to ensure the continued usefulness of the data schemata
as the data model on Wikidata evolves,
will also be developed.

\section{Details}

Data schemata can be expressed using the \gls{shex} Language \cite{Prud'hommeaux:2014:SER:2660517.2660523},
referring to the \gls{rdf} output format of Wikidata.
The output of the inference process would therefore be a \gls{shex} schema in one of the \gls{shex} syntaxes, e.\,g. ShExC.

These schemata could be stored on Wikidata itself,
where they would be open to users amending and improving them.
Of course, the average Wikidata editor cannot be expected to be familiar with \gls{shex},
but since several community members are already investigating the use of \gls{shex} on Wikidata,
a collaborative effort seems possible.

\section{Administrative details}

The work will be performed at the \gls{aifb},
in collaboration with Prof. Harald Sack and Dr. Maria Koutraki,
under supervision of \textit{TBD}.

The work will be conducted in cooperation with \gls{wmde},
the non-profit organization driving the technological development of Wikidata.
The author, Lucas Werkmeister, is currently employed as an intern at \gls{wmde},
with this work being part of his work there.

\section{Schema Inference vs. Ontology Learning}

The goal of this work is explicitly the inference of schemata of the data representation on Wikidata,
not the learning of an ontology on the data.
When an item is in violation of a schema,
for instance because an expected statement is missing,
this implies nothing more than the simple fact of the violation:
it is possible that the statement should be added
(in this case, an ontology might have inferred the statement),
but it is equally possible that something else should be changed about the item so that the missing statement is no longer expected,
or that the schema simply does not apply to the item and nothing should be done at all.

\section{Related Work}

Schema inference has previously been done on XML \cite{Bex:2007:IXS:1325851.1325964} and JSON \cite{json-inference}.
On \gls{rdf}, there has been more focus on ontology learning \cite{Zhou2007}, a related but different problem.
RDF2Graph \cite{vanDam2015} can recover the structure of an \gls{rdf} resource and express it in \gls{shex},
but generally assumes a well-maintained, unified ontology,
which is not always the case on Wikidata.


\gls{shex} has previously been used to validate \gls{rdf} data in \cite{SOLBRIG201790}.

\bibliographystyle{plain}
\bibliography{expose}

\end{document}
